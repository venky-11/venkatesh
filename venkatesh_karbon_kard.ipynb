{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7670fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating a web scraper using Node.js, Puppeteer, and JavaScript involves several steps. Puppeteer is a headless browser automation library that can be used to navigate web pages, interact with page elements, and extract data.\n",
    "\n",
    "Here's a basic example to get you started:\n",
    "\n",
    "Install Node.js and npm if you haven't already.\n",
    "\n",
    "Create a new Node.js project and install Puppeteer:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "npm init -y\n",
    "npm install puppeteer\n",
    "Create a JavaScript file (e.g., scraper.js) and implement the web scraper:\n",
    "javascript\n",
    "Copy code\n",
    "const puppeteer = require('puppeteer');\n",
    "const fs = require('fs');\n",
    "const csvWriter = require('csv-writer').createObjectCsvWriter;\n",
    "\n",
    "const companies = [\n",
    "  // List of companies goes here\n",
    "];\n",
    "\n",
    "async function scrapeLinkedIn(company) {\n",
    "  const browser = await puppeteer.launch();\n",
    "  const page = await browser.newPage();\n",
    "\n",
    "  try {\n",
    "    // Navigate to LinkedIn\n",
    "    await page.goto('https://www.linkedin.com', { waitUntil: 'domcontentloaded' });\n",
    "\n",
    "    // Login to LinkedIn (you may need to handle this part manually)\n",
    "\n",
    "    // Search for the company on LinkedIn\n",
    "    await page.type('input[placeholder=\"Search\"]', `${company} LinkedIn`);\n",
    "    await page.keyboard.press('Enter');\n",
    "    await page.waitForNavigation();\n",
    "\n",
    "    // Extract company details\n",
    "    const companyLink = await page.$eval('a.search-result__result-link', (link) => link.href);\n",
    "\n",
    "    // Navigate to the company's LinkedIn page\n",
    "    await page.goto(companyLink, { waitUntil: 'domcontentloaded' });\n",
    "    await page.waitForSelector('.pv4 div a');\n",
    "\n",
    "    // Extract CEO, CXO, CFO details\n",
    "    const personnelDetails = await page.evaluate(() => {\n",
    "      const titles = ['CEO', 'CFO', 'CXO']; // Add other titles as needed\n",
    "      const details = [];\n",
    "\n",
    "      for (const title of titles) {\n",
    "        const element = document.querySelector(`section.pv-profile-section .pv-entity__position-group-pager:first-of-type .pv-entity__position-group:first-of-type .pv-entity__summary-info h3:contains('${title}')`);\n",
    "        if (element) {\n",
    "          const name = element.textContent.trim();\n",
    "          const emailElement = document.querySelector(`section.pv-profile-section .pv-entity__position-group-pager:first-of-type .pv-entity__position-group:first-of-type .pv-entity__summary-info span[aria-label=\"Email\"]`);\n",
    "          const email = emailElement ? emailElement.textContent.trim() : '';\n",
    "          const phoneElement = document.querySelector(`section.pv-profile-section .pv-entity__position-group-pager:first-of-type .pv-entity__position-group:first-of-type .pv-entity__summary-info span[aria-label=\"Phone\"]`);\n",
    "          const phone = phoneElement ? phoneElement.textContent.trim() : '';\n",
    "          details.push({ title, name, email, phone });\n",
    "        }\n",
    "      }\n",
    "\n",
    "      return details;\n",
    "    });\n",
    "\n",
    "    return {\n",
    "      company,\n",
    "      websiteLink: companyLink,\n",
    "      personnelDetails,\n",
    "    };\n",
    "  } catch (error) {\n",
    "    console.error('Error:', error);\n",
    "  } finally {\n",
    "    await browser.close();\n",
    "  }\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  const scrapedData = [];\n",
    "\n",
    "  for (const company of companies) {\n",
    "    const result = await scrapeLinkedIn(company);\n",
    "    scrapedData.push(result);\n",
    "  }\n",
    "\n",
    "  // Write the data to a CSV file\n",
    "  const csvWriterInstance = csvWriter({\n",
    "    path: 'output.csv',\n",
    "    header: [\n",
    "      { id: 'company', title: 'Company' },\n",
    "      { id: 'websiteLink', title: 'Website Link' },\n",
    "      { id: 'personnelDetails.title', title: 'Title' },\n",
    "      { id: 'personnelDetails.name', title: 'Name' },\n",
    "      { id: 'personnelDetails.email', title: 'Email' },\n",
    "      { id: 'personnelDetails.phone', title: 'Phone' },\n",
    "    ],\n",
    "  });\n",
    "\n",
    "  await csvWriterInstance.writeRecords(scrapedData);\n",
    "}\n",
    "\n",
    "main();\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
